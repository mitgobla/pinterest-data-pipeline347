# Databricks notebook source
# MAGIC %md
# MAGIC # Cleaning Geo Dataframe
# MAGIC
# MAGIC Using PySpark to transform and clean the Dataframes generated by JSON files from S3 bucket.

# COMMAND ----------

# MAGIC %run "./1. Imports and Common Functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ## Cleaning the Geo Dataframe
# MAGIC
# MAGIC Combine `latitude` and `longitude` columns into a single array column called `coordinates`
# MAGIC
# MAGIC Then drop the original `latitude` and `longitude` columns.

# COMMAND ----------

df_geo = df_geo.withColumn("coordinates", Funcs.array("latitude", "longitude"))
df_geo = df_geo.drop("latitude", "longitude")
display(df_geo)

# COMMAND ----------

# MAGIC %md
# MAGIC Cast `timestamp` column from `string` to `timestamp` type.

# COMMAND ----------

df_geo = df_geo.withColumn("timestamp", Funcs.to_timestamp("timestamp"))
display(df_geo)

# COMMAND ----------

# MAGIC %md
# MAGIC Reorder columns to the specified order.

# COMMAND ----------

df_geo = df_geo.select("ind", "country", "coordinates", "timestamp")
display(df_geo)
