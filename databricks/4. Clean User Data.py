# Databricks notebook source
# MAGIC %md
# MAGIC # Cleaning User Dataframe
# MAGIC
# MAGIC Using PySpark to transform and clean the Dataframes generated by JSON files from S3 bucket.

# COMMAND ----------

# MAGIC %run "./1. Imports and Common Functions"

# COMMAND ----------

# MAGIC %md
# MAGIC ## Load Dataframes
# MAGIC
# MAGIC Create User Dataframe from the JSON files in the S3 bucket.

# COMMAND ----------

df_user = create_topic_dataframe("user")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Cleaning the User Dataframe
# MAGIC
# MAGIC Creating a column `user_name` that concatinates the columns `first_name` and `last_name`
# MAGIC
# MAGIC Then dropping `first_name` and `last_name` columns.

# COMMAND ----------

df_user = df_user.withColumn("user_name", Funcs.concat("first_name", Funcs.lit(" "), "last_name"))
df_user = df_user.drop("first_name", "last_name")
display(df_user)

# COMMAND ----------

# MAGIC %md
# MAGIC Casting `date_joined` to a timestamp column.

# COMMAND ----------

df_user = df_user.withColumn("date_joined", Funcs.to_timestamp("date_joined"))
display(df_user)

# COMMAND ----------

# MAGIC %md
# MAGIC Reordering the columns to the specified order

# COMMAND ----------

df_user = df_user.select("ind", "user_name", "age", "date_joined")
display(df_user)
